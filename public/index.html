<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>VoxTalk ‚Äì TTS Hook (No Deltas)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="icon" href="data:,">
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="app">
    <h1>Talk to VoxTalk</h1>
    <div class="status-row">
      <span id="statusPill" class="pill idle">Idle</span>
    </div>

    <button id="pttBtn" aria-pressed="false" title="Push to Talk"></button>
    <div class="hint">Click to talk</div>

    <audio id="remote" autoplay playsinline></audio>
  </div>

  <!-- Print button (simple) -->
  <button id="printBtn" class="print-btn" title="Print page">üñ®Ô∏è Print</button>

  <script>
    // --- Elements
    const pttBtn   = document.getElementById('pttBtn');
    const rtAudio  = document.getElementById('remote');
    const statusEl = document.getElementById('statusPill');
    const printBtn = document.getElementById('printBtn');

    // --- Speaking UI toggle (color + subtle ring)
    function setSpeaking(on) {
      pttBtn.classList.toggle('speaking', on);
      statusEl.textContent = on ? 'Speaking' : 'Idle';
      statusEl.classList.toggle('speaking', on);
      statusEl.classList.toggle('idle', !on);
    }

    // --- Mic/PTT
    let pc = null;
    let micTrack = null;
    let listening = false;

    pttBtn.onclick = () => {
      listening = !listening;
      if (micTrack) micTrack.enabled = listening;
      pttBtn.classList.toggle('active', listening);
    };

    printBtn.onclick = () => window.print();

    // --- Realtime init (no deltas used anywhere)
    async function initRealtime() {
      try {
        console.info('[VoxTalk] Requesting session‚Ä¶');
        const s = await fetch('/session', { method: 'POST' });
        const { client_secret, model, voice } = await s.json();

        // Peer connection
        pc = new RTCPeerConnection();

        // Mic (disabled until user clicks)
        const mic = await navigator.mediaDevices.getUserMedia({ audio: true });
        micTrack = mic.getTracks()[0];
        micTrack.enabled = false;
        pc.addTrack(micTrack, mic);

        // Play incoming AI audio
        pc.ontrack = (ev) => {
          console.info('[VoxTalk] ontrack: AI audio stream attached');
          rtAudio.srcObject = ev.streams[0];
        };

        // IMPORTANT: listen for the DATA CHANNEL that OpenAI creates
        pc.ondatachannel = (e) => {
          const dc = e.channel;
          console.info('[VoxTalk] ondatachannel label =', dc.label || '(no label)');
          dc.onmessage = (msg) => {
            let evt;
            try { evt = JSON.parse(msg.data); } catch { return; }

            // Clean lifecycle hooks (ChatGPT equivalent: tts_start / tts_stop)
            if (evt.type === 'response.audio.started') {
              console.info('[VoxTalk] TTS started');
              setSpeaking(true);
            }
            if (evt.type === 'response.audio.done' || evt.type === 'output_audio_buffer.stopped') {
              console.info('[VoxTalk] TTS done');
              setSpeaking(false);
            }
          };
        };

        // Offer ‚Üí Answer
        const offer = await pc.createOffer({ offerToReceiveAudio: true });
        await pc.setLocalDescription(offer);

        const r = await fetch(
          `https://api.openai.com/v1/realtime?model=${encodeURIComponent(model)}&voice=${voice}`,
          {
            method: 'POST',
            headers: {
              'Authorization': `Bearer ${client_secret.value}`,
              'Content-Type' : 'application/sdp'
            },
            body: offer.sdp
          }
        );

        const answer = { type: 'answer', sdp: await r.text() };
        await pc.setRemoteDescription(answer);

        console.info('[VoxTalk] Realtime connected. Waiting for TTS events‚Ä¶');
      } catch (err) {
        console.error('[VoxTalk] Realtime init failed:', err);
      }
    }

    initRealtime();
  </script>
</body>
</html>
