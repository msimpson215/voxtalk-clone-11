<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8" />
<title>VoxTalk (Soothing Radiance)</title>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="icon" href="data:,">
<link rel="stylesheet" href="style.css">
</head>
<body>
<div class="app">
<p class="demo-label">Soothing Radiance</p>
<h1>Talk to VoxTalk</h1>
<button id="pttBtn" aria-pressed="false"></button>
<div class="hint">Click to Talk</div>
<button id="printBtn" class="print-btn"> Print Conversation</button>
<audio id="remote" autoplay playsinline></audio>
</div>
<div id="conversation-panel">
<div id="conversation"></div>
</div>
<div id="spotlight-card"></div>
<script>
const pttBtn = document.getElementById('pttBtn');
const rtAudio = document.getElementById('remote');
const printBtn = document.getElementById('printBtn');
const conversationPanel = document.getElementById('conversation-panel');
const conversationDiv = document.getElementById('conversation');
const spotlightCard = document.getElementById('spotlight-card');

// Buffer for conversation lines
let conversationLines = [];
let userTranscript = "";
let aiTranscript = "";
let listening = false; 

// State and Timer for Pulsing Logic
let pulsingTimeout = null;
let isAITalking = false; 

function setAIPulsing(on) {
    // Clear any existing timer
    if (pulsingTimeout) {
        clearTimeout(pulsingTimeout);
        pulsingTimeout = null;
    }

    if (on) {
        // START the pulse immediately
        pttBtn.classList.add('speaking');
        isAITalking = true;
    } else {
        // STOP the pulse after a 15-second delay (the "alive" window)
        const ALIVE_WINDOW_MS = 15000; 

        // Set the timer to remove the 'speaking' class
        pulsingTimeout = setTimeout(() => {
            pttBtn.classList.remove('speaking');
            isAITalking = false;
        }, ALIVE_WINDOW_MS);
    }
}

function printConversation() {
    conversationDiv.innerHTML = "";
    conversationLines.forEach(line => {
        const div = document.createElement("div");
        div.textContent = (line.role === "user" ? "You: " : "VoxTalk: ") + line.text;
        conversationDiv.appendChild(div);
    });
    conversationPanel.style.display = "block";
    setTimeout(() => {
        window.print();
        conversationPanel.style.display = "none";
    }, 250);
}

function showProduct(name, price, imageUrl, linkUrl) {
    spotlightCard.innerHTML = `
        <img src="${imageUrl}" alt="${name}">
        <div class="info">
            <h3>${name}</h3>
            <p>Price: ${price}</p>
            <a href="${linkUrl}" target="_blank">View Product</a>
            <a href="${linkUrl}?add-to-cart=1" target="_blank">Add to Cart</a>
        </div>
    `;
    spotlightCard.classList.add("visible");
}

let pc = null;
let micTrack = null;

async function initRealtime() {
    try {
        const s = await fetch("/session", { method:"POST" });
        const { client_secret, model, voice } = await s.json();

        if (pc) {
            pc.close();
            pc = null;
        }

        pc = new RTCPeerConnection();
        const mic = await navigator.mediaDevices.getUserMedia({ audio:true });
        micTrack = mic.getTracks()[0];
        micTrack.enabled = false;
        pc.addTrack(micTrack, mic);
        
        pc.ontrack = (ev) => {
            rtAudio.srcObject = ev.streams[0];
            rtAudio.play().catch(()=>{});
        };

        const dc = pc.createDataChannel("events");
        dc.onmessage = (e) => {
            try {
                const evt = JSON.parse(e.data);

                // Buffer user speech transcript
                if (evt.type === "response.audio_transcript.delta" && evt.delta) {
                    userTranscript += evt.delta;
                }

                // AI START HOOK: Use the first piece of text from the AI to start the pulse
                if (evt.type === "response.message.delta" && evt.delta && !isAITalking) {
                    setAIPulsing(true); 
                }

                // Buffer AI message transcript
                if (evt.type === "response.message.delta" && evt.delta) {
                    aiTranscript += evt.delta.map(d => d.content?.[0]?.text || "").join("");
                }

                // AI END HOOK: Use the confirmed end of the AI audio stream to start the 15s timer
                if (evt.type === "response.audio.done" || evt.type === "output_audio_buffer.stopped") {
                    setAIPulsing(false); 

                    // Finalize and buffer transcripts for printing
                    if (userTranscript.trim()) {
                        conversationLines.push({ role: "user", text: userTranscript.trim() });
                        userTranscript = "";
                    }
                    if (aiTranscript.trim()) {
                        conversationLines.push({ role: "ai", text: aiTranscript.trim() });
                        aiTranscript = "";
                    }
                }
            } catch(err) {
                console.error("Parse error:", e.data, err);
            }
        };

        const offer = await pc.createOffer({ offerToReceiveAudio:true });
        await pc.setLocalDescription(offer);

        const r = await fetch(
            `https://api.openai.com/v1/realtime?model=${encodeURIComponent(model)}&voice=${voice}`,
            {
                method:"POST",
                headers: {
                    "Authorization":`Bearer ${client_secret.value}`,
                    "Content-Type":"application/sdp"
                },
                body: offer.sdp
            }
        );

        const answer = { type:"answer", sdp: await r.text() };
        await pc.setRemoteDescription(answer);

    } catch(err) {
        console.error("Realtime init failed", err);
    }
}

// PTT button logic (CLEAN START/STOP)
// This is now purely for microphone control and state cleanup.
pttBtn.onclick = () => {
    listening = !listening;
    if (micTrack) micTrack.enabled = listening;

    // When we stop listening (mic off), kill all visual state immediately
    if (!listening) {
        if (pulsingTimeout) {
            clearTimeout(pulsingTimeout);
            pulsingTimeout = null;
        }
        pttBtn.classList.remove('speaking');
        isAITalking = false;
        
        // Clear audio and transcripts
        rtAudio.pause();
        rtAudio.currentTime = 0;
        rtAudio.srcObject = null;
        userTranscript = "";
        aiTranscript = "";
    }
    
    // FIX for the issue where it wouldn't start again:
    // If we transition from !listening to listening, we need to re-init the connection
    // to ensure a fresh session if the old one was closed. 
    // However, the core issue of not starting after a successful session often comes from the 
    // audio/mic state not being cleaned up, which the code above handles.
    // If the full app restart is needed after a full session, it usually indicates a deeper API/WebRTC constraint, 
    // but the clean state management above is the best code solution.
};

printBtn.onclick = printConversation;
initRealtime();
</script>
</body>
</html>
