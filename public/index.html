<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>VoxTalk Halo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="app">
    <p class="demo-label">Halo Test</p>
    <h1>Talk to VoxTalk</h1>

    <button id="pttBtn" aria-pressed="false"></button>
    <div class="hint">Click to Talk</div>
    <audio id="remote" autoplay playsinline></audio>
  </div>

  <script>
    const pttBtn = document.getElementById('pttBtn');
    const rtAudio = document.getElementById('remote');
    let pc, micTrack;

    function setSpeaking(on) {
      pttBtn.classList.toggle('speaking', on);
    }

    async function initRealtime() {
      const s = await fetch("/session", { method:"POST" });
      const { client_secret, model, voice } = await s.json();

      pc = new RTCPeerConnection();
      const mic = await navigator.mediaDevices.getUserMedia({ audio:true });
      micTrack = mic.getTracks()[0];
      micTrack.enabled = false;
      pc.addTrack(micTrack, mic);

      // as soon as we get audio stream, hook into its events
      pc.ontrack = (ev)=> {
        rtAudio.srcObject = ev.streams[0];

        // halo on when audio actually starts playing
        rtAudio.onplay = ()=> setSpeaking(true);

        // halo off when audio ends or is paused
        rtAudio.onpause = ()=> setSpeaking(false);
        rtAudio.onended = ()=> setSpeaking(false);
      };

      const dc = pc.createDataChannel("events");
      dc.onmessage = (e)=> {
        const evt = JSON.parse(e.data);
        if (evt.type === "response.audio.started") console.log("AI started speaking");
        if (evt.type === "response.audio.done" || evt.type === "output_audio_buffer.stopped") console.log("AI stopped speaking");
      };

      const offer = await pc.createOffer({ offerToReceiveAudio:true });
      await pc.setLocalDescription(offer);

      const r = await fetch(
        `https://api.openai.com/v1/realtime
